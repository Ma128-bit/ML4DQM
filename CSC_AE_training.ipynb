{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1882e1f-3d63-4d5f-b96b-83590737bd3e",
   "metadata": {},
   "source": [
    "# AutoEncoder for CSC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2deffb-fb4d-4c36-a3ec-449b5943421d",
   "metadata": {},
   "source": [
    "###  Import and function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a7bc5a-6e8f-4de7-a06c-3970ee69996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math, time, copy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from plot_functions import *\n",
    "from ResNet import *\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"You are using:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16792cc8-81cf-4b6c-bfba-13817d8037ff",
   "metadata": {},
   "source": [
    "### Loading Data from previous strep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de98c83-c12f-4b8b-aa79-d5bf1fec7052",
   "metadata": {},
   "outputs": [],
   "source": [
    "me = \"hRHGlobalm3\"\n",
    "data = np.load(me+'_files.npz')\n",
    "selected_chamber = data[\"imgs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a52e8f-be8c-4506-b128-c619084a42fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_matrix = np.mean(selected_chamber, axis=0)\n",
    "dim = len(selected_chamber)\n",
    "Show2Dimg(mean_matrix)\n",
    "mean_matrix[mean_matrix == 0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df60c4f0-7855-488e-84f6-b7d39f40ddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_list = [(torch.tensor(m, dtype=torch.float32)).unsqueeze(0) for m in selected_chamber]\n",
    "print(tensor_list[0].size())\n",
    "training_dim = int(dim*0.85)\n",
    "training_tensor = tensor_list[:training_dim]\n",
    "validation_tensor = tensor_list[training_dim:]\n",
    "print(f\"Data ({len(selected_chamber)}) = Training sample ({len(training_tensor)}) + Validation sample ({len(validation_tensor)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aec386e-dd25-4777-996c-d7ae7e59d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for k in range(len(validation_tensor)):\n",
    "#    for i in range(50, 100):\n",
    "#        for j in range(50, 100):\n",
    "#            if (i<1.3*j-15 and i>0.7*j+15):\n",
    "#                validation_tensor[k][0][i][j]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bfa9c8-6aa4-4ce0-8f81-72ac4191d977",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = TensorDataset(*training_tensor)\n",
    "validation_dataset = TensorDataset(*validation_tensor)\n",
    "\n",
    "training_loader = DataLoader(dataset=training_tensor, \n",
    "                             batch_size=64, \n",
    "                             num_workers=10, \n",
    "                             shuffle=True)\n",
    "\n",
    "validation_loader = DataLoader(dataset=validation_tensor, \n",
    "                               batch_size=32,\n",
    "                               num_workers=10, \n",
    "                               shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb24b34c-78a2-40c7-8cff-1449c8bf0cb7",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63976684-d106-4611-b542-52b7c385dfbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_size = list(tensor_list[0].squeeze(0).size())\n",
    "ae = ResNetAE(1, 3, [16, 32, 64], img_size=img_size).to(device)\n",
    "optimizer = optim.Adam(ae.parameters(), lr=5.e-4)\n",
    "train_loss, val_loss =[], []\n",
    "epochs = 50\n",
    "for e in range(epochs):\n",
    "    epoch = e+1\n",
    "    print(f'***** Training Epoch {epoch} *****')\n",
    "    # Run training\n",
    "    ae.train()\n",
    "    now = time.time()\n",
    "    tloss=[]\n",
    "    for i, figure in enumerate(training_loader):\n",
    "        X = figure.to(device)\n",
    "        #X = X\n",
    "        optimizer.zero_grad()\n",
    "        Xreco = ae(X)\n",
    "        \n",
    "        #if i==1:\n",
    "        #    PLots_in_training(X, Xreco)\n",
    "        #print(X)\n",
    "        #loss = F.l1_loss(Xreco, X)\n",
    "        #loss = F.mse_loss(Xreco, X) smooth_l1_loss\n",
    "        loss = F.l1_loss(Xreco, X)\n",
    "        tloss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i%4 ==0:\n",
    "            print(f'>> [{i}/{len(training_loader)}] Train loss:{loss.item()}')\n",
    "    train_loss.append(np.mean(tloss))\n",
    "    now = time.time() - now\n",
    "    s = '>> Training time: %.2f min in %d steps'%( now/60, len(training_loader))\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e52a12d-41c8-4ac1-8485-ddd1e60af955",
   "metadata": {},
   "source": [
    "### Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0468df0c-e0a2-47b3-bfd6-caaf259e052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ae, \"model.pth\")\n",
    "#model = torch.load('model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140fc476-516e-4e24-88c4-e40801438706",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, img in enumerate(validation_loader):\n",
    "    img = img.to(device)\n",
    "    #img = img*(img<10).float()\n",
    "    reco_img = ae(img)\n",
    "    print(f\"***** batch {i} *****\")\n",
    "    #k=0\n",
    "    #if True:\n",
    "    for k in range(4):\n",
    "        print(f\" >> Example {k}\")\n",
    "        print(' >> Original image:')\n",
    "        Show2Dimg(img[k][0].cpu().numpy())\n",
    "        print(' >> AE-reco image:')\n",
    "        Show2Dimg(reco_img[k][0].detach().cpu().numpy())\n",
    "        print(' >> Normalized loss map:')\n",
    "        img_loss = F.l1_loss(reco_img[k], img[k], reduction='none')[0].detach().cpu().numpy()\n",
    "        #Show2Dimg(img_loss/mean_matrix)\n",
    "        out = img_loss/mean_matrix\n",
    "        fig = plt.figure(figsize =(8, 8))\n",
    "        plt.imshow(out, cmap=plt.cm.jet, vmin=0, vmax=3)\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.colorbar()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f61dce-048d-40ed-adc9-18f002ee2968",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
